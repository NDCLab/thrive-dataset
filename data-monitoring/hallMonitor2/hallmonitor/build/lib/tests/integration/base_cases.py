import json
import logging
import os
import re
import shutil
import subprocess
import tempfile
from abc import ABC, abstractmethod
from dataclasses import dataclass

import pandas as pd
import pytest

from hallmonitor.hmutils import get_new_redcaps


@dataclass
class ExpectedError:
    """
    Represents an expected error with its type, details, and expected occurrence.

    Attributes:
        error_type (str): The type of the error (e.g., "Empty file").
        info_regex (str): A regex pattern to match the error details.
        multiplicity (int): The number of times this error is expected to occur. Default is 1.
    """

    error_type: str
    info_regex: str
    multiplicity: int = 1


class TestCase(ABC):
    """
    A base class for generating test cases by modifying a base dataset.

    Attributes:
        basedir (str): The base directory containing test case data.
        case_name (str): The name of the test case.
        description (str): A description of the test case.
        conditions (list[str]): A list of conditions applied in the test case.
        expected_output (str): A description of the expected output for the test case.
        case_dir (str): The directory where the test case files will be written.
    """

    SUBJECT_ID = 3000001

    def __init__(self, basedir):
        """
        Initialize a TestCase.

        Args:
            basedir (str): The base directory containing test case data.
        """
        self.basedir = basedir
        self.sub_id = TestCase.SUBJECT_ID
        TestCase.SUBJECT_ID += 1  # increment sub ID for each test case
        self.case_dir = os.path.join(basedir, self.case_name)

    @classmethod
    def run_test_case(cls, request: pytest.FixtureRequest):
        """
        Runs a test case by creating an instance of the test case class, generating the test case, and validating it.

        Args:
            request (pytest.FixtureRequest): Pytest request object to fetch fixtures dynamically.
        """
        tmp_path = request.getfixturevalue("tmp_path")
        persist_dir = request.getfixturevalue("persist_dir")

        test_case = cls(tmp_path)
        dest_dir = os.path.join(str(persist_dir), test_case.case_name)

        test_case.generate()
        if persist_dir:
            TestCase.persist_files(test_case.case_dir, dest_dir)

        test_case.validate()
        if persist_dir:
            TestCase.persist_files(test_case.case_dir, dest_dir)

    @staticmethod
    def persist_files(src_dir, dest_dir):
        """
        Copies files from the source directory to the destination directory.

        Args:
            src_dir (str): The source directory containing the temporary files.
            dest_dir (str): The destination directory where files will be copied.
        """
        if os.path.exists(dest_dir):
            # previous runs may have filled dest_dir, we want to start fresh
            shutil.rmtree(dest_dir)
        shutil.copytree(src_dir, dest_dir)
        print(f"Test files persisted to: {dest_dir}")

    def get_base_files(self):
        """
        Create a base subject directory with standard files for testing, then return
        those files in a representation generated by read_files().
        """
        tmpdir = tempfile.TemporaryDirectory()
        basedir = os.path.join(tmpdir.name, self.case_name)
        os.makedirs(basedir)

        # add metadata file
        metadata = {
            "description": "Unmodified test data.",
            "subject": f"sub-{self.sub_id}",
        }
        with open(os.path.join(basedir, "metadata.json"), "w") as f:
            json.dump(metadata, f)

        # -- set up standard files --
        # (stored in "checked order": sub/ses/dtype)

        checked_data_dir = os.path.join(
            basedir, "sourcedata", "checked", f"sub-{self.sub_id}"
        )
        os.makedirs(checked_data_dir)

        ses_runs = [("s1", "r1"), ("s2", "r1"), ("s3", "r1")]

        datatypes = set()
        # set up files in checked directory
        for ses, run in ses_runs:
            # set up session/run directory
            sr_dir = os.path.join(checked_data_dir, f"{ses}_{run}")
            os.makedirs(sr_dir)

            # set up psychopy data

            datatypes.add("psychopy")
            psychopy_dir = os.path.join(sr_dir, "psychopy")
            os.makedirs(psychopy_dir)

            psychopy_var = "arrow-alert-v1-1_psychopy"
            psychopy_base = f"sub-{self.sub_id}_{psychopy_var}_{ses}_{run}_e1"

            psychopy_log = psychopy_base + ".log"
            psychopy_psydat = psychopy_base + ".psydat"
            for filename in {psychopy_log, psychopy_psydat}:
                with open(os.path.join(psychopy_dir, filename), "w") as f:
                    f.write("psychopy data")

            psychopy_csv = psychopy_base + ".csv"
            # ID in CSV matches subject ID
            df = pd.DataFrame([{"id": self.sub_id}])
            df.to_csv(os.path.join(psychopy_dir, psychopy_csv), index=False)

            # set up eeg data

            datatypes.add("eeg")
            eeg_dir = os.path.join(sr_dir, "eeg")
            os.makedirs(eeg_dir)

            eeg_var = "all_eeg"
            eeg_base = f"sub-{self.sub_id}_{eeg_var}_{ses}_{run}_e1"

            eeg_data = eeg_base + ".eeg"
            eeg_vhdr = eeg_base + ".vhdr"
            eeg_vmrk = eeg_base + ".vmrk"

            with open(os.path.join(eeg_dir, eeg_data), "w") as f:
                f.write("eeg data")

            # match .vmrk and .eeg files
            with open(os.path.join(eeg_dir, eeg_vhdr), "w") as f:
                vhdr_lines = [
                    f"MarkerFile={eeg_vmrk}",
                    f"DataFile={eeg_data}",
                ]
                f.write("\n".join(vhdr_lines))

            # just match .eeg file
            with open(os.path.join(eeg_dir, eeg_vmrk), "w") as f:
                f.write(f"DataFile={eeg_data}")

            # set up audacity data

            datatypes.add("audacity")
            audacity_dir = os.path.join(sr_dir, "audacity")
            os.makedirs(audacity_dir)

            audacity_zip_gpg = f"sub-{self.sub_id}_all_audacity_{ses}_{run}_e1.zip.gpg"
            with open(os.path.join(audacity_dir, audacity_zip_gpg), "w") as f:
                f.write("audacity data")

            # set up zoom data

            datatypes.add("zoom")
            zoom_dir = os.path.join(sr_dir, "zoom")
            os.makedirs(zoom_dir)

            zoom_zip_gpg = f"sub-{self.sub_id}_all_zoom_{ses}_{run}_e1.zip.gpg"
            with open(os.path.join(zoom_dir, zoom_zip_gpg), "w") as f:
                f.write("zoom data")

            # set up digi data

            datatypes.add("digi")
            digi_dir = os.path.join(sr_dir, "digi")
            os.makedirs(digi_dir)

            digi_zip_gpg = f"sub-{self.sub_id}_all_digi_{ses}_{run}_e1.zip.gpg"
            with open(os.path.join(digi_dir, digi_zip_gpg), "w") as f:
                f.write("digi data")

        # copy checked directory files to raw directory
        raw_data_dir = os.path.join(basedir, "sourcedata", "raw")
        os.makedirs(raw_data_dir)
        for ses, run in ses_runs:
            ses_run = f"{ses}_{run}"
            for dtype in datatypes:
                dtype_dir = os.path.join(raw_data_dir, ses_run, dtype)
                os.makedirs(dtype_dir)
                src_path = os.path.join(checked_data_dir, ses_run, dtype)
                dest_path = os.path.join(dtype_dir, f"sub-{self.sub_id}")
                subprocess.check_call(["cp", "-r", src_path, dest_path])

        # -- set up sourcedata/pending-qa/ directory --

        pending_qa_dir = os.path.join(basedir, "sourcedata", "pending-qa")
        os.makedirs(pending_qa_dir)

        # set up empty QA checklist

        checklist_path = os.path.join(pending_qa_dir, "qa-checklist.csv")
        mock_checklist = pd.DataFrame(
            columns=[
                "datetime",
                "user",
                "identifier",
                "deviationString",
                "subject",
                "dataType",
                "encrypted",
                "suffix",
                "qa",
                "localMove",
            ]
        )
        mock_checklist.to_csv(checklist_path, index=False)

        # -- set up data-monitoring/ directory --

        data_monitoring_dir = os.path.join(basedir, "data-monitoring")
        os.makedirs(data_monitoring_dir)

        # set up validated file record
        validated_files_path = os.path.join(
            data_monitoring_dir, "validated-file-record.csv"
        )
        validated_files = pd.DataFrame(
            columns=[
                "datetime",
                "user",
                "identifier",
                "subject",
                "dataType",
                "encrypted",
                "suffix",
            ]
        )
        validated_files.to_csv(validated_files_path, index=False)

        # set up pending/ directory and files

        pending_dir = os.path.join(data_monitoring_dir, "pending")
        os.makedirs(pending_dir)

        timestamp = "2024-01-01_12-30"

        pending_files_path = os.path.join(pending_dir, f"pending-files-{timestamp}.csv")
        pending_files = pd.DataFrame(
            columns=[
                "datetime",
                "user",
                "passRaw",
                "identifier",
                "subject",
                "dataType",
                "encrypted",
                "suffix",
                "errorType",
                "errorDetails",
            ]
        )
        pending_files.to_csv(pending_files_path, index=False)

        pending_errors_path = os.path.join(
            pending_dir, f"pending-errors-{timestamp}.csv"
        )
        pending_errors = pd.DataFrame(
            columns=[
                "datetime",
                "user",
                "identifier",
                "subject",
                "dataType",
                "encrypted",
                "suffix",
                "errorType",
                "errorDetails",
            ]
        )
        pending_errors.to_csv(pending_errors_path, index=False)

        # set up data dictionary and "latest" data dictionary

        dd_dir = os.path.join(data_monitoring_dir, "data-dictionary")
        os.makedirs(dd_dir)

        dd_path = os.path.join(dd_dir, "central-tracker_datadict.csv")
        latest_dd_path = os.path.join(dd_dir, "central-tracker_datadict_latest.csv")
        mock_dd = [
            {
                "variable": "id",
                "dataType": "id",
                "description": "Participant ID",
                "detail": "The participant ID is specific to this study, and is auto-assigned by REDCap.",
                "allowedSuffix": "NA",
                "measureUnit": "Integer",
                "allowedValues": "[3000000,3009999],[3080000,3089999],[3090000,3099999]",
                "valueInfo": "One ID per participant (eligible and ineligible)",
                "provenance": f'file: "{self.case_name}consent"; variable: "record_id"',
                "expectedFileExt": "NA",
            },
            {
                "variable": "consent",
                "dataType": "consent",
                "description": "Participant consent status",
                "detail": 'When data is transferred from raw to checked, value of 1 is assigned based on the value of "consent_complete" or "consentes_complete" == "2"(indicating participant consented), otherwise 0.',
                "allowedSuffix": "NA",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, no data exists | 1, data exists",
                "provenance": f'file: "{self.case_name}consent"; variable: ""',
                "expectedFileExt": "NA",
            },
            {
                "variable": "assent",
                "dataType": "assent",
                "description": "Participant assent status",
                "detail": 'When data is transferred from raw to checked, value of 1 is assigned based on the value of "assent_complete"=="2" (indicating participant assented), otherwise 0.',
                "allowedSuffix": "NA",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, no data exists | 1, data exists",
                "provenance": f'file: "{self.case_name}consent"; variable: ""',
                "expectedFileExt": "NA",
            },
            {
                "variable": "arrow-alert-v1-1_psychopy",
                "dataType": "psychopy",
                "description": "arrow-alert-v1-1_psychopy task status",
                "detail": 'When data is transferred from raw to checked, value of 1 is assigned if "arrow-alert-v1-1" file exists, otherwise 0.',
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, no data exists | 1, data exists",
                "provenance": "direct-psychopy",
                "expectedFileExt": ".psydat, .csv, .log",
            },
            {
                "variable": "arrow-alert-v1-2_psychopy",
                "dataType": "psychopy",
                "description": "arrow-alert-v1-2_psychopy task status",
                "detail": 'When data is transferred from raw to checked, value of 1 is assigned if "arrow-alert-v1-2" file exists, otherwise 0.',
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, no data exists | 1, data exists",
                "provenance": "direct-psychopy",
                "expectedFileExt": ".psydat, .csv, .log",
            },
            {
                "variable": "arrow-alert_psychopy",
                "dataType": "combination",
                "description": "arrow-alert_psychopy task status",
                "detail": "When updatetracker is run, value of 1 is assigned if either of the variables specificed for a given allowedSuffix = 1, otherwise a value of 0 is assigned.",
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, no data exists | 1, data exists",
                "provenance": 'variables: "arrow-alert-v1-1_psychopy","arrow-alert-v1-2_psychopy"',
                "expectedFileExt": "NA",
            },
            {
                "variable": "all_audacity",
                "dataType": "audacity",
                "description": "Audacity data status",
                "detail": "When hallMonitor is run, value of 1 is assigned if data already exists in checked, otherwise 0.",
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, no data exists | 1, data exists",
                "provenance": "direct-audacity",
                "expectedFileExt": ".zip.gpg",
            },
            {
                "variable": "all_zoom",
                "dataType": "zoom",
                "description": "Zoom data status",
                "detail": "When hallMonitor is run, value of 1 is assigned if data already exists in checked, otherwise 0.",
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, no data exists | 1, data exists",
                "provenance": "direct-zoom",
                "expectedFileExt": ".zip.gpg",
            },
            {
                "variable": "all_eeg",
                "dataType": "eeg",
                "description": "Brain Vision EEG data status",
                "detail": "When data is transferred from raw to checked, value of 1 is assigned if data exists, otherwise 0.",
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, no data exists | 1, data exists",
                "provenance": "direct-eeg",
                "expectedFileExt": ".eeg, .vmrk, .vhdr",
            },
            {
                "variable": "all_digi",
                "dataType": "digi",
                "description": "Digi data status",
                "detail": "When hallMonitor is run, value of 1 is assigned if data already exists in checked, otherwise 0.",
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, no data exists | 1, data exists",
                "provenance": "direct-digi",
                "expectedFileExt": ".zip.gpg",
            },
            {
                "variable": "iqs_status",
                "dataType": "visit_status",
                "description": "Status of participant's IQS visit (attended vs not attended)",
                "detail": "Value of 1 is assigned if the participant attended the IQS visit, otherwise 0.",
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, participant has not attended IQS | 1, participant has attended IQS",
                "provenance": 'file: "iqsclinician"; variable: "iqsclchecklist"',
                "expectedFileExt": "NA",
            },
            {
                "variable": "bbs_status",
                "dataType": "visit_status",
                "description": "Status of participant's BBS visit (attended vs not attended)",
                "detail": "Value of 1 is assigned if the participant attended the BBS visit, otherwise 0.",
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, participant has not attended BBS | 1, participant has attended BBS",
                "provenance": 'file: "bbsra"; variable: "bbsradebrief"; id: "bbsratrk_acid"',
                "expectedFileExt": "NA",
            },
            {
                "variable": "iqs_data",
                "dataType": "visit_data",
                "description": "Status of participant's IQS data (present on HPC or not)",
                "detail": "Value of 1 is assigned if IQS data exists, otherwise 0.",
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, IQS data does not exist | 1, IQS data exists",
                "provenance": 'file: "iqsclinician"',
                "expectedFileExt": "NA",
            },
            {
                "variable": "bbs_data",
                "dataType": "visit_data",
                "description": "Status of participant's BBS data (present on HPC or not)",
                "detail": "Value of 1 is assigned if all BBS data exists, otherwise 0.",
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, At least some BBS data does not exist | 1, All BBS data exists",
                "provenance": 'variables: "arrow-alert_psychopy","all_audacity","all_zoom","all_eeg","all_digi"',
                "expectedFileExt": "NA",
            },
            {
                "variable": "abq",
                "dataType": "redcap_data",
                "description": "ABQ questionnaire status",
                "detail": 'When data is transferred from raw to checked, value of 1 is assigned based on the value of "{questionnaire name}_complete"!=NULL (indicating participant began questionnaire), otherwise 0.',
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, no data exists | 1, data exists",
                "provenance": 'file: "bbschild"; variable: ""',
                "expectedFileExt": "NA",
            },
            {
                "variable": "psb",
                "dataType": "redcap_data",
                "description": "PSB questionnaire status",
                "detail": 'When data is transferred from raw to checked, value of 1 is assigned based on the value of "{questionnaire name}_complete"!=NULL (indicating participant began questionnaire), otherwise 0.',
                "allowedSuffix": "s1_r1_e1, s2_r1_e1, s3_r1_e1",
                "measureUnit": "Logical",
                "allowedValues": "NA, 0, 1",
                "valueInfo": "NA, status unknown | 0, no data exists | 1, data exists",
                "provenance": 'file: "iqschild"; variable: ""',
                "expectedFileExt": "NA",
            },
        ]
        mock_dd = pd.DataFrame(mock_dd)

        mock_dd["encrypted"] = False
        encrypted_dtypes = {"audacity", "zoom", "digi"}
        mock_dd.loc[mock_dd["dataType"].isin(encrypted_dtypes), "encrypted"] = True

        mock_dd.to_csv(dd_path, index=False)
        mock_dd.to_csv(latest_dd_path, index=False)

        # set up minimum necessary REDCaps

        redcap_dir = os.path.join(basedir, "sourcedata", "checked", "redcap")
        os.makedirs(redcap_dir)

        # basic "consent" REDCap (not associated with a ses/run)
        rc_consent_path = os.path.join(redcap_dir, self.build_rc_name("consent"))
        consent_row = {
            "record_id": self.sub_id,
            # English variables
            "consent_complete": 2,
            "assent_complete": 2,
            # Spanish variables
            "consentes_complete": 2,
            "assentes_complete": 2,
        }
        consent_df = pd.DataFrame([consent_row])
        consent_df.to_csv(rc_consent_path, index=False)

        # set up identical REDCap files for all session/run combinations,
        # with appropriate substitutions to file content and names
        for ses, run in ses_runs:
            # basic "iqsclinician" REDCap
            rc_iqsclinician_path = os.path.join(
                redcap_dir, self.build_rc_name("iqsclinician", ses, run)
            )
            iqsclinician_row = {
                "record_id": self.sub_id,
                f"iqsclchecklist_{ses}_{run}_e1_complete": 2,
            }
            iqsclinician_df = pd.DataFrame([iqsclinician_row])
            iqsclinician_df.to_csv(rc_iqsclinician_path, index=False)

            # basic "bbsRA" REDCap
            rc_bbsra_path = os.path.join(
                redcap_dir, self.build_rc_name("bbsRA", ses, run)
            )
            bbsra_row = {
                "record_id": 1,
                f"bbsradebrief_{ses}_{run}_e1_complete": 2,
                f"bbsratrk_acid_{ses}_{run}_e1": self.sub_id,
            }
            bbsra_df = pd.DataFrame([bbsra_row])
            bbsra_df.to_csv(rc_bbsra_path, index=False)

            # basic "bbschild" REDCap
            rc_bbschild_path = os.path.join(
                redcap_dir, self.build_rc_name("bbschild", ses, run)
            )
            bbschild_row = {
                "record_id": self.sub_id,
                f"abq_{ses}_{run}_e1_complete": 2,
            }
            bbschild_df = pd.DataFrame([bbschild_row])
            bbschild_df.to_csv(rc_bbschild_path, index=False)

            # basic "iqschild" REDCap
            rc_iqschild_path = os.path.join(
                redcap_dir, self.build_rc_name("iqschild", ses, run)
            )
            iqschild_row = {
                "record_id": self.sub_id,
                f"psb_{ses}_{run}_e1_complete": 2,
            }
            iqschild_df = pd.DataFrame([iqschild_row])
            iqschild_df.to_csv(rc_iqschild_path, index=False)

        # set up empty central tracker

        # handle record_id, consent, and assent variables separately
        tracker_cols = {"id", "consent", "assent"}

        var_df = mock_dd[~mock_dd["variable"].isin(tracker_cols)]
        for _, row in var_df.iterrows():
            variable = str(row["variable"])
            suffixes = str(row["allowedSuffix"]).split(",")
            for suffix in suffixes:
                tracker_cols.add(f"{variable}_{suffix.strip()}")

        case_name = os.path.basename(self.case_name)
        tracker_path = os.path.join(
            data_monitoring_dir, f"central-tracker_{case_name}.csv"
        )

        tracker_df = pd.DataFrame(columns=list(tracker_cols))
        tracker_df.to_csv(tracker_path, index=False)

        # save file output before cleanup
        base_files = self.read_files(basedir)
        # since we didn't use a context manager, we call cleanup() ourselves
        tmpdir.cleanup()

        return base_files

    def build_rc_name(self, rc_stem: str, ses: str = "", run: str = ""):
        """
        Constructs a REDCap name string for the given test case.

        Args:
            rc_stem (str): The stem of the REDCap name.
            ses (str, optional): The session identifier. Defaults to an empty string.
            run (str, optional): The run identifier. Defaults to an empty string.

        Returns:
            str: The constructed REDCap name string in the format
                 "{case_name}{rc_stem}{ses}{run}_DATA_{RC_TIMESTAMP}.csv".
        """
        RC_TIMESTAMP = "2024-01-01_1230"
        return f"{self.case_name}{rc_stem}{ses}{run}_DATA_{RC_TIMESTAMP}.csv"

    def get_paths(self, base_dir: str):
        """Retrieve a list of relative file paths from a base directory.

        Args:
            base_dir (str): The path to the base directory to scan for files.

        Raises:
            FileNotFoundError: If the provided `base_dir` does not exist or is not a directory.

        Returns:
            list[str]: A list of relative file paths found in the base directory.
        """
        if not os.path.isdir(base_dir):
            raise FileNotFoundError(f"{base_dir} does not exist, or is not a directory")

        paths = []
        for root, _, files in os.walk(base_dir):
            for filename in files:
                paths.append(os.path.relpath(os.path.join(root, filename), base_dir))

        return paths

    def read_files(self, base_dir: str):
        """Read all files in a base directory and return their contents in a dictionary.

        Args:
            base_dir (str): The path to the base directory to read files from.

        Raises:
            FileNotFoundError: If the provided `base_dir` does not exist or is not a directory.

        Returns:
            dict[str,str]: A dictionary where keys are relative file paths and values are file contents
        """
        new_files = {}

        if not os.path.isdir(base_dir):
            raise FileNotFoundError(f"{base_dir} does not exist, or is not a directory")

        for root, _, files in os.walk(base_dir):
            for filename in files:
                file_path = os.path.join(root, filename)
                rel_path = os.path.relpath(file_path, base_dir)

                with open(file_path, "r") as f:
                    content = f.read()

                new_files[rel_path] = content

        return new_files

    def write_files(self, files: dict[str, str]):
        """
        Write the modified files to the test case directory.

        Args:
            files (dict[str,str]): A dictionary where keys are relative paths to files and values are file contents.
        """
        for rel_path, content in files.items():
            full_path = os.path.join(self.case_dir, rel_path)
            os.makedirs(os.path.dirname(full_path), exist_ok=True)

            try:
                with open(full_path, "w") as f:
                    f.write(content)
            except IsADirectoryError:  # some test cases require empty directories
                continue

    def build_path(self, ses: str, datatype: str, filename: str, is_raw=False):
        """
        Constructs a file path by joining the base directory with session, datatype, and filename.

        Args:
            ses (str): The session identifier.
            datatype (str): The datatype.
            filename (str): The name of the file.
            is_raw (bool): Whether a raw filepath should be generated. Defaults to False.

        Returns:
            str: The constructed file path, rooted at "sourcedata".
        """
        if is_raw:
            return os.path.join(
                "sourcedata",
                "raw",
                ses,
                datatype,
                f"sub-{self.sub_id}",
                filename,
            )
        else:
            return os.path.join(
                "sourcedata",
                "checked",
                f"sub-{self.sub_id}",
                ses,
                datatype,
                filename,
            )

    def replace_file_name(self, base_files, old_name, new_name):
        """
        Searches for a file by its basename in the given dictionary of files and replaces its name if found.

        Args:
            base_files (dict[str, str]): A dictionary where keys are relative file paths and values are file contents.
            old_name (str): The basename of the file to search for.
            new_name (str): The new basename to replace the old one with.

        Returns:
            bool: True if the file was found and replaced; False otherwise.
        """
        for relpath in base_files:
            if os.path.basename(relpath) == old_name:
                old_dir = os.path.dirname(relpath)
                new_relpath = os.path.join(old_dir, new_name)
                base_files[new_relpath] = base_files.pop(relpath)
                return True

        return False

    def remove_file(self, base_files, file):
        """
        Removes a file from the given dictionary of files if it exists.

        Args:
            base_files (dict[str, str]): A dictionary where keys are relative file paths and values are file contents.
            file (str): The basename of the file to remove.

        Returns:
            bool: True if the file was found and removed, False otherwise.
        """
        path = ""
        for relpath in base_files:
            if os.path.basename(relpath) == file:
                path = relpath
                break
        if path:
            del base_files[path]
            return True
        else:
            return False

    def write_metadata(self):
        """
        Write metadata for the test case to a JSON file.

        Metadata includes the test case name, description, conditions, and expected output.
        """
        metadata = {
            "test_case": str(self.case_name),
            "description": self.description,
            "conditions": self.conditions,
            "expected_output": self.expected_output,
            "subject": f"sub-{self.sub_id}",
        }
        with open(os.path.join(self.case_dir, "metadata.json"), "w") as f:
            json.dump(metadata, f, indent=4)

    @property
    @abstractmethod
    def case_name(self) -> str:
        """Return the test case's name."""
        pass

    @property
    @abstractmethod
    def behavior_to_test(self) -> str:
        """Return a short description of the behavior being tested."""
        pass

    @property
    @abstractmethod
    def conditions(self) -> list[str]:
        """Return a list of conditions applied in the test case."""
        pass

    @abstractmethod
    def modify(self, base_files: dict[str, str]) -> dict[str, str]:
        """
        Apply modifications to the base files.

        Args:
            base_files (dict[str,str]): A dictionary where keys are filenames and values are file contents.

        Returns:
            dict[str,str]: A dictionary where keys are filenames and values are modified file contents.
        """
        pass

    def generate(self):
        """
        Generate the test case by reading the base subject, applying modifications,
        and writing the modified files and metadata.
        """
        base_files = self.get_base_files()
        modified_files = self.modify(base_files)
        self.write_files(modified_files)
        self.write_metadata()

    def get_standard_args(self):
        """
        Create and return a mock namespace object representing the standard command-line arguments.

        Returns:
            MockNamespace: An object containing the standard arguments to hallMonitor 2.0.
        """

        class MockNamespace:
            dataset = self.case_dir
            child_data = None
            ignore_before_date = None
            legacy_exceptions = False
            no_color = False
            no_qa = False
            # we typically do not want to gather logging output
            output = os.devnull
            checked_only = False
            raw_only = False
            verbose = False
            quiet = False
            replace = None
            map = None

        return MockNamespace()

    def run_validate_data(self, *args, **kwargs):
        """
        Run validate_data() on the generated data directory and collect errors.

        Returns:
            pd.DataFrame: A DataFrame containing the errors reported by validate_data.
        """
        from hallmonitor.hallmonitor import validate_data

        # set up a logger to save hallMonitor output
        logger = logging.getLogger(f"{self.case_name}_logger")
        logger.setLevel(logging.ERROR)
        logger.propagate = False

        pending = validate_data(
            logger,
            dataset=self.case_dir,
            *args,
            use_legacy_exceptions=False,
            is_raw=False,
            **kwargs,
        )

        pending_df = pd.DataFrame(pending)
        # 'datetime' and 'user' columns do not matter for verifying output
        cols_to_drop = ["datetime", "user"]
        if all(c in pending_df.columns for c in cols_to_drop):
            pending_df.drop(columns=cols_to_drop, inplace=True)

        return pending_df

    def run_qa_validation(self, *args, **kwargs) -> str:
        """Run qa_validation() on the generated data directory.

        Returns:
            str: A string containing error text, if any error was raised during execution.
        """
        from hallmonitor.hallmonitor import qa_validation

        logger = logging.getLogger(f"{self.case_name}_logger")
        logger.setLevel(logging.ERROR)
        logger.propagate = False

        error_text = ""
        try:
            qa_validation(logger, dataset=self.case_dir, *args, **kwargs)
        except Exception as err:
            error_text = str(err)

        return error_text

    def run_update_tracker(
        self,
        child: bool,
        session: str,
        dataset: str = None,
        redcaps: list[str] = [],
        passed_id_list: list[str] = [],
        failed_id_list: list[str] = [],
    ) -> str:
        """Run the update_tracker script on the generated data directory.

        Returns:
            str: A string containing error text, if any error was raised during execution.
        """
        from hallmonitor.updatetracker import update_tracker

        if dataset is None:
            dataset = self.case_dir

        if passed_id_list == failed_id_list == []:
            expected_vars = [
                "arrow-alert-v1-1_psychopy",
                "arrow-alert-v1-2_psychopy",
                "all_audacity",
                "all_zoom",
                "all_eeg",
                "all_digi",
            ]
            expected_sre = ["s1_r1_e1", "s2_r1_e1", "s3_r1_e1"]

            passed_id_list = [
                f"sub-{self.sub_id}_{var}_{sre}"
                for var in expected_vars
                for sre in expected_sre
            ]

        if not redcaps:
            redcap_dir = os.path.join(self.case_dir, "sourcedata", "checked", "redcap")
            redcaps = [
                rc
                for rc in get_new_redcaps(redcap_dir)
                if re.fullmatch(rf".*{session}(r\d+)?_DATA.*", os.path.basename(rc))
                or not re.fullmatch(r".*s\d+.*", os.path.basename(rc))
            ]

        try:
            update_tracker.main(
                dataset,
                redcaps,
                session,
                child,
                passed_id_list,
                failed_id_list,
            )
        except Exception as err:
            raise RuntimeError("update_tracker exited with error") from err

    @abstractmethod
    def validate(self):
        """
        Validate the test case as appropriate for its type.
        """
        pass


class ValidationTestCase(TestCase):
    """
    Base class for test cases associated with the data validation stage.
    """

    case_name = "ValidationTestCase"
    description = "Handles errors related to data validation."
    conditions = []
    expected_output = "Correct error generated for data validation issues."

    @property
    def behavior_to_test(self) -> str:
        return "Tests for errors related to pending errors."

    @abstractmethod
    def get_expected_errors(self) -> list[ExpectedError]:
        """
        Generate a list of ExpectedError instances for this object.

        This method defines the expected errors that may occur,
        using dynamically generated file paths and error messages
        specific to the current object's state. Each error is
        represented as an ExpectedError instance.

        Returns:
            list[ExpectedError]: A list of ExpectedError objects
            encapsulating the error type and associated message.
        """
        pass

    def compare_errors(self, generated_errors_df: pd.DataFrame):
        """
        Compare the generated errors DataFrame with the gold standard errors.

        Args:
            generated_errors_df (pd.DataFrame): A DataFrame containing the errors generated by validate_data.

        Raises:
            AssertionError: If there are differences between the generated errors and the gold standard errors.
        """
        expected_errors = self.get_expected_errors()

        # check for missing errors

        if generated_errors_df.empty:
            # can't put backslashes in f-strings
            escape_char = "\\"
            # we may have no errors; in this case, all expected errors are missing
            missing = [
                f"{error.error_type}: {error.info_regex.replace(escape_char, '')}"
                + f" (missing {error.multiplicity})"
                for error in expected_errors
            ]

        else:
            missing = []
            for error in expected_errors:
                matching_errors = generated_errors_df[
                    (generated_errors_df["errorType"] == error.error_type)
                    & (
                        generated_errors_df["errorDetails"].str.fullmatch(
                            error.info_regex
                        )
                    )
                ]
                if len(matching_errors.index) < error.multiplicity:
                    n_missing = error.multiplicity - len(matching_errors.index)
                    missing.append(
                        f"{error.error_type}: {error.info_regex.replace('\\', '')} (missing {n_missing})"
                    )

        # check for extraneous errors
        extra = []
        for _, row in generated_errors_df.iterrows():
            is_expected = any(
                row["errorType"] == error.error_type
                and re.fullmatch(error.info_regex, row["errorDetails"])
                for error in expected_errors
            )
            if not is_expected:
                extra.append(f'{row["errorType"]}: {row["errorDetails"]}')

        # construct failure message
        fail_reason = ""
        if missing:
            fail_reason += "Missing errors:\n" + "\n".join(missing) + "\n"
        if extra:
            fail_reason += "Extra errors:\n" + "\n".join(extra) + "\n"

        if fail_reason:
            raise AssertionError(fail_reason)

    def validate(self):
        errors_df = self.run_validate_data()
        self.compare_errors(errors_df)


class QATestCase(TestCase):
    """
    Base class for errors associated with the quality assurance (QA) stage.
    """

    case_name = "QATestCase"
    description = "Handles errors related to quality assurance."
    conditions = []
    expected_output = "Correct error generated for quality assurance issues."

    @property
    def behavior_to_test(self) -> str:
        return "Tests for errors related to quality assurance."


class TrackerTestCase(TestCase):
    """
    Base class for errors associated with the update-tracker script.
    """

    case_name = "TrackerTestCase"
    description = "Handles errors related to central tracker updates."
    conditions = []
    expected_output = "Correct error generated for central tracker issues."

    @property
    def behavior_to_test(self) -> str:
        return "Tests for errors related to quality assurance."
